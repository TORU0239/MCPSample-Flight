import OpenAI from "openai";
import Anthropic from "@anthropic-ai/sdk";
import type { ChatMessage } from "./types.js";
import type { ContentBlock, TextBlock } from "@anthropic-ai/sdk/resources/messages";

// LLM í”„ë¡œë°”ì´ë” íƒ€ì…
type LLMProvider = "openai" | "anthropic";

// í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
const PROVIDER = (process.env.LLM_PROVIDER || "openai").toLowerCase() as LLMProvider;

// API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
const openai = process.env.OPENAI_API_KEY
  ? new OpenAI({ apiKey: process.env.OPENAI_API_KEY })
  : undefined;

const anthropic = process.env.ANTHROPIC_API_KEY
  ? new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })
  : undefined;

/**
 * LLMê³¼ ëŒ€í™”í•˜ê¸°
 * @param messages ëŒ€í™” ë‚´ì—­
 * @returns AI ì‘ë‹µ í…ìŠ¤íŠ¸
 */
export async function chat(messages: ChatMessage[]): Promise<string> {
  // ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ëŒ€í™” ë©”ì‹œì§€ ë¶„ë¦¬
  const systemMessage = messages.find((m) => m.role === "system");
  const conversationMessages = messages.filter((m) => m.role !== "system");

  console.log(`ğŸ¤– LLM í˜¸ì¶œ (${PROVIDER})`);

  try {
    if (PROVIDER === "anthropic" && anthropic) {
      // Claude ì‚¬ìš©
      const response = await anthropic.messages.create({
        model: process.env.ANTHROPIC_MODEL || "claude-3-5-sonnet-20241022",
        max_tokens: 800,
        system: systemMessage?.content,
        messages: conversationMessages.map((m) => ({
          role: m.role === "assistant" ? "assistant" : "user",
          content: m.content,
        })),
      });

      // Claude ì‘ë‹µ ì¶”ì¶œ - íƒ€ì… ê°€ë“œ ì´ìš©
      function isTextBlock(block: ContentBlock): block is TextBlock {
        return block.type === "text";
      }

      const text = response.content.filter(isTextBlock).map((c) => c.text).join("");

      console.log(`âœ… Claude ì‘ë‹µ ë°›ìŒ (${text.length}ì)`);
      return text;
    } else if (openai) {
      // OpenAI GPT ì‚¬ìš©
      const response = await openai.chat.completions.create({
        model: process.env.OPENAI_MODEL || "gpt-4o-mini",
        messages: messages.map(m => ({
          role: m.role,
          content: m.content
        })),
        temperature: 0.2,
        max_tokens: 800,
      });

      const text = response.choices[0]?.message?.content || "";
      console.log(`âœ… GPT ì‘ë‹µ ë°›ìŒ (${text.length}ì)`);
      return text;
    } else {
      throw new Error(`LLM í”„ë¡œë°”ì´ë”ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (í˜„ì¬: ${PROVIDER})`);
    }
  } catch (error) {
    console.error("âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨:", error);
    throw error;
  }
}

// ì‹œì‘ ì‹œ ì„¤ì • í™•ì¸
export function validateLLMConfig(): void {
  if (PROVIDER === "anthropic" && !anthropic) {
    throw new Error("ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.");
  }
  if (PROVIDER === "openai" && !openai) {
    throw new Error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.");
  }

  console.log(`âœ… LLM ì„¤ì • ì™„ë£Œ: ${PROVIDER}`);
}
